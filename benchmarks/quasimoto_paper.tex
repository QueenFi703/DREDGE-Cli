\documentclass[11pt,twocolumn]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}

\title{\textbf{Quasimoto: Learnable Wave Function Architectures for Non-Stationary Signal Processing}}

\author{
    QueenFi703\\
    \texttt{https://github.com/QueenFi703/DREDGE}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We introduce \textit{Quasimoto}, a family of neural network architectures based on learnable wave functions with Gaussian envelopes and phase modulation for continuous function representation. Unlike global frequency models such as SIREN or Random Fourier Features, our approach enables localized activation and adaptive frequency warping, providing significant advantages for signals with spatial or temporal irregularities. We present comprehensive benchmarks comparing Quasimoto against industry-standard baselines on non-stationary chirp signals with phase discontinuities. Furthermore, we demonstrate remarkable scalability by extending the architecture to 4D (3 spatial + 1 temporal) and 6D (5 spatial + 1 temporal) spatiotemporal domains, as well as a complex-valued interference basis variant. Our results show that Quasimoto maintains exceptional parameter efficiency while scaling from 1D (144 parameters) to 6D (127 parameters) representations, achieving competitive performance with up to 17× fewer parameters than baseline methods.
\end{abstract}

\section{Introduction}

Continuous function representation is a fundamental problem in machine learning, with applications ranging from neural rendering and implicit neural representations to signal processing and scientific computing. Recent work has explored sinusoidal activation functions (SIREN)~\cite{sitzmann2020siren} and random Fourier features~\cite{rahimi2007random} as powerful tools for learning high-frequency functions. However, these approaches suffer from a key limitation: they employ \textit{global} frequency priors that treat the entire input space uniformly.

For many real-world signals---such as seismic events, medical imaging anomalies, or financial market irregularities---the interesting phenomena are highly \textit{localized} in space and time. A signal may be mostly smooth but contain high-frequency bursts in specific regions. Global frequency models face a dilemma: either overfit the noise everywhere or blur the localized irregularities.

We propose \textit{Quasimoto}, a novel neural architecture family that addresses this challenge through:

\begin{enumerate}
    \item \textbf{Gaussian Envelopes}: Waves only activate in specific spatial/temporal windows via $\exp(-\frac{1}{2}\frac{(x-vt)^2}{\sigma^2})$, enabling different wave units to specialize in distinct regions.
    
    \item \textbf{Phase Modulation}: Local frequency warping through $\epsilon\cos(\lambda x)$ allows waves to adaptively stretch and compress their periods to fit non-stationary data.
    
    \item \textbf{Scalability}: Natural extension from 1D to arbitrary dimensions (4D, 6D demonstrated) with consistent mathematical framework.
    
    \item \textbf{Complex-Valued Variant}: Interference basis using $\exp(i\sum k_i x_i)$ for applications requiring phase-sensitive representations.
\end{enumerate}

Our contributions include:

\begin{itemize}
    \item A novel wave function architecture with localized envelopes and learnable phase modulation
    \item Extensions to 4D and 6D spatiotemporal domains
    \item Complex-valued interference basis with learned superposition
    \item Comprehensive benchmarks against SIREN and Random Fourier Features
    \item Open-source implementation and reproducible experiments
\end{itemize}

\section{Related Work}

\subsection{Implicit Neural Representations}

Implicit neural representations have gained significant attention for their ability to represent continuous functions. Coordinate-based networks~\cite{mildenhall2020nerf} map spatial coordinates to output values, enabling applications in 3D reconstruction, novel view synthesis, and beyond.

\textbf{SIREN}~\cite{sitzmann2020siren} introduced sinusoidal activation functions with specialized initialization schemes, demonstrating superior performance for fitting high-frequency signals compared to ReLU networks. However, SIREN's global sinusoids struggle with localized irregularities.

\textbf{Random Fourier Features (RFF)}~\cite{rahimi2007random} project inputs through fixed random frequencies, providing an efficient kernel approximation. While computationally attractive, RFF cannot adapt its frequency distribution to the data.

\subsection{Localized Representations}

Wavelet networks~\cite{zhang1992wavelet} and Gabor networks~\cite{lee1996gabor} explored localized basis functions but typically used fixed, handcrafted parameterizations. Our work extends this direction with fully learnable localized wave functions.

\section{Method}

\subsection{QuasimotoWave: Core Architecture}

The fundamental building block of our architecture is the \textit{QuasimotoWave}, a learnable wave function with controlled phase irregularity:

\begin{equation}
\psi(x,t) = A \cos(\Phi) \cdot E \cdot M
\end{equation}

where:

\begin{align}
\Phi &= kx - \omega t \quad \text{(wave phase)} \\
E &= \exp\left(-\frac{1}{2}\frac{(x - vt)^2}{\sigma^2}\right) \quad \text{(Gaussian envelope)} \\
M &= \sin(\phi + \epsilon\cos(\lambda x)) \quad \text{(phase modulation)}
\end{align}

\textbf{Learnable Parameters} (8 per wave):
\begin{itemize}
    \item $A$: amplitude
    \item $k$: wave number (spatial frequency)
    \item $\omega$: angular frequency (temporal frequency)
    \item $v$: envelope velocity
    \item $\sigma$: envelope width (log-parameterized for stability)
    \item $\phi$: phase offset
    \item $\epsilon$: modulation strength
    \item $\lambda$: modulation frequency
\end{itemize}

\subsection{Ensemble Architecture}

For practical applications, we employ an ensemble of $n$ QuasimotoWave units:

\begin{equation}
f(x,t) = W \cdot [\psi_1(x,t), \psi_2(x,t), \ldots, \psi_n(x,t)]^T
\end{equation}

where $W \in \mathbb{R}^{1 \times n}$ is a learnable linear projection. For an ensemble of size $n=16$, the total parameter count is:
\begin{equation}
\text{params} = 8n + n = 9n = 144
\end{equation}

\subsection{4D Extension: Spatiotemporal Volumes}

For volumetric spatiotemporal data $(x,y,z,t)$, we extend QuasimotoWave to 4D:

\begin{equation}
\psi(x,y,z,t) = A \cos(\Phi_{4D}) \cdot E_{4D} \cdot M_{4D}
\end{equation}

\begin{align}
\Phi_{4D} &= k_x x + k_y y + k_z z - \omega t \\
E_{4D} &= \exp\left(-\frac{1}{2\sigma^2}\sum_{i \in \{x,y,z\}} (i - v_i t)^2\right) \\
M_{4D} &= \sin(\phi + \epsilon\cos(\lambda_x x + \lambda_y y + \lambda_z z))
\end{align}

\textbf{Parameters}: 13 per wave (3 wave numbers, 3 velocities, 3 modulation frequencies, plus 4 shared).

\subsection{6D Extension: Hyperspace}

For 5D spatial + temporal dimensions $(x_1,x_2,x_3,x_4,x_5,t)$:

\begin{equation}
\psi(x_1,\ldots,x_5,t) = A \cos(\Phi_{6D}) \cdot E_{6D} \cdot M_{6D}
\end{equation}

\begin{align}
\Phi_{6D} &= \sum_{i=1}^5 k_i x_i - \omega t \\
E_{6D} &= \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^5 (x_i - v_i t)^2\right) \\
M_{6D} &= \sin\left(\phi + \epsilon\cos\left(\sum_{i=1}^5 \lambda_i x_i\right)\right)
\end{align}

\textbf{Parameters}: 17 per wave. For a 6-wave ensemble: $17 \times 6 + 6 = 127$ parameters.

\subsection{Complex-Valued Interference Basis}

For applications requiring phase-sensitive representations (quantum mechanics, wave interference), we introduce a complex-valued variant:

\begin{equation}
\psi_j(x,t) = A_j \cdot \exp(i\Phi_j) \cdot E \cdot M_j
\end{equation}

where:
\begin{align}
\Phi_j &= \sum_{i=1}^d k_{ji} x_i - \omega_j t \quad \text{(complex phase)} \\
E &= \frac{1}{\mathcal{N}}\exp\left(-\frac{1}{2}\sum_{i=1}^d \frac{(x_i - v_i t)^2}{\sigma_i^2}\right) \\
M_j &= \sin(\phi_j + \epsilon_j\cos(\sum_{i=1}^d \lambda_{ji} x_i))
\end{align}

The output is a learned superposition of real and imaginary parts:

\begin{equation}
f(x,t) = W \cdot [\text{Re}(\psi_1), \ldots, \text{Re}(\psi_n), \text{Im}(\psi_1), \ldots, \text{Im}(\psi_n)]^T
\end{equation}

where $W \in \mathbb{R}^{m \times 2n}$ maps complex wave fields to real outputs.

\textbf{Key Features}:
\begin{itemize}
    \item Anisotropic envelopes ($\sigma_i$ per dimension)
    \item Shared envelope across fields
    \item Learned complex superposition weights
    \item $2dn + 4n + d + d + 2nm$ parameters
\end{itemize}

\section{Experiments}

\subsection{Experimental Setup}

\textbf{Test Signal}: We evaluate on a "glitchy chirp"---a non-stationary signal that combines:
\begin{itemize}
    \item Chirp: $\sin(0.5x^2) \exp(-0.1x^2)$ (accelerating frequency)
    \item Glitch: High-frequency burst at 50-55\% position
\end{itemize}

This signal is designed to stress-test localization vs. global frequency priors.

\textbf{Baselines}:
\begin{itemize}
    \item SIREN: 3 layers, 64 hidden units, $w_0=30$
    \item Random Fourier Features: 128 features, $\sigma=5$
\end{itemize}

\textbf{Training}: Adam optimizer, learning rate $10^{-3}$, 1000-2000 epochs.

\subsection{1D Benchmark Results}

\begin{table}[h]
\centering
\caption{1D Benchmark Performance (1000 epochs)}
\begin{tabular}{lrr}
\toprule
\textbf{Architecture} & \textbf{Params} & \textbf{Final Loss} \\
\midrule
Quasimoto (n=8) & 73 & 0.00814 \\
Interference Basis & 66 & 0.00830 \\
RFF & 256 & 0.00590 \\
SIREN & 1,153 & 0.000015 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis}: 
\begin{itemize}
    \item SIREN achieves lowest loss through global optimization
    \item Quasimoto demonstrates regional specialization with 17× fewer parameters
    \item Interference basis shows competitive performance with complex wave interference
\end{itemize}

\subsection{4D Spatiotemporal Benchmark}

\textbf{Task}: Fit 3D Gaussian with sinusoidal structure on $15^3$ grid (3,375 points).

\textbf{Result}: Quasimoto-4D (112 parameters, 8 waves) achieves 0.00034 loss in 300 epochs.

\subsection{6D Hyperspace Benchmark}

\textbf{Task}: Fit 5D spatial signal on $8^5$ grid (32,768 points).

\textbf{Result}: Quasimoto-6D (127 parameters, 6 waves) achieves:
\begin{itemize}
    \item Final loss: 0.00282 (500 epochs)
    \item Sample MSE: 0.000025 (first 100 points)
\end{itemize}

\subsection{Scalability Analysis}

\begin{table}[h]
\centering
\caption{Architecture Scalability}
\begin{tabular}{lrrr}
\toprule
\textbf{Arch.} & \textbf{Points} & \textbf{Params} & \textbf{Loss} \\
\midrule
1D & 1,000 & 144 & 0.0105 \\
4D & 8,000 & 112 & 0.00034 \\
6D & 32,768 & 127 & 0.00282 \\
\bottomrule
\end{tabular}
\end{table}

The architecture maintains consistent parameter efficiency across dimensions, with parameter count \textit{decreasing} in some cases due to ensemble size adjustments.

\section{Analysis}

\subsection{Why Quasimoto Works}

\textbf{Localized Activation}: The Gaussian envelope $E$ constrains each wave to activate only in specific regions, enabling:
\begin{itemize}
    \item Regional specialization (different waves focus on different areas)
    \item Prevention of global interference
    \item Natural handling of localized irregularities
\end{itemize}

\textbf{Adaptive Frequencies}: The phase modulation $M$ allows waves to warp their periods locally:
\begin{equation}
\text{effective period} \propto \frac{1}{k(1 + \epsilon\lambda\sin(\lambda x))}
\end{equation}

\textbf{Factorized Representation}: In high dimensions, independent per-dimension parameters avoid the curse of dimensionality that affects fully-connected architectures.

\subsection{Comparison with SIREN}

\begin{itemize}
    \item \textbf{SIREN}: Global sinusoids $\sin(Wx + b)$ → Uniform treatment
    \item \textbf{Quasimoto}: Localized waves $\cos(kx) \cdot \exp(-(x-vt)^2/2\sigma^2)$ → Adaptive treatment
\end{itemize}

Trade-off: SIREN achieves lower loss through dense parameterization; Quasimoto provides interpretability and parameter efficiency.

\subsection{Comparison with RFF}

\begin{itemize}
    \item \textbf{RFF}: Fixed frequencies $\sin(Bx), \cos(Bx)$ where $B$ is frozen
    \item \textbf{Quasimoto}: Learnable frequencies $k, \omega, \lambda$
\end{itemize}

Advantage: Quasimoto adapts frequency distribution to data structure.

\section{Applications}

\subsection{Medical Imaging (4D)}
4D CT/MRI scans capture organ motion over time. Quasimoto-4D can model:
\begin{itemize}
    \item Cardiac cycles with localized envelope tracking heart walls
    \item Respiratory motion with traveling wave patterns
\end{itemize}

\subsection{Quantum Mechanics (Interference Basis)}
The complex-valued interference basis naturally represents:
\begin{itemize}
    \item Quantum wave functions $\psi \in \mathbb{C}$
    \item Interference patterns from superposition
    \item Time evolution via Schrödinger equation
\end{itemize}

\subsection{Multi-Modal Sensor Fusion (6D)}
For RGB-D + thermal + audio + time:
\begin{equation}
(x_1, x_2, x_3, x_4, x_5, t) = (R, G, B, \text{depth}, \text{thermal}, t)
\end{equation}

Quasimoto-6D can learn correlations across modalities with localized coupling.

\subsection{Seismic Signal Processing}
Earthquake detection benefits from:
\begin{itemize}
    \item Localized envelopes detecting events in specific time windows
    \item Phase modulation capturing P-wave/S-wave characteristics
    \item Low parameter count enabling deployment on edge devices
\end{itemize}

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{enumerate}
    \item \textbf{Convergence Speed}: Quasimoto typically requires more epochs than SIREN for equivalent loss (though with fewer parameters).
    
    \item \textbf{Global Smooth Functions}: For perfectly smooth signals without irregularities, SIREN's global approach may be more efficient.
    
    \item \textbf{Initialization Sensitivity}: Gaussian envelope parameters require careful initialization for high dimensions.
\end{enumerate}

\subsection{Future Directions}

\begin{enumerate}
    \item \textbf{Attention Mechanisms}: Weight different waves based on input location for adaptive ensemble selection.
    
    \item \textbf{Hierarchical Grids}: Multi-resolution representations combining coarse and fine-scale waves.
    
    \item \textbf{Physics-Informed Learning}: Incorporate wave equation constraints for physical system modeling.
    
    \item \textbf{Beyond 6D}: Extend to even higher dimensions with sparse representations or factorized tensors.
    
    \item \textbf{Continuous Convolutions}: Define convolution operators for wave functions enabling translation equivariance.
\end{enumerate}

\section{Conclusion}

We introduced Quasimoto, a family of neural architectures based on learnable wave functions with Gaussian envelopes and phase modulation. Our key contributions include:

\begin{itemize}
    \item A novel approach to continuous function representation through localized, adaptive waves
    \item Demonstrated scalability from 1D to 6D with consistent parameter efficiency
    \item Complex-valued interference basis for phase-sensitive applications
    \item Comprehensive benchmarks showing competitive performance with significantly fewer parameters
\end{itemize}

The mathematical elegance of wave functions—combining locality, frequency adaptation, and physical interpretability—provides a promising direction for future research in implicit neural representations. We release our implementation as open-source software to enable reproducible research and further exploration of wave-based architectures.

\section*{Acknowledgments}

This work was supported by the open-source community and builds on foundational research in implicit neural representations, sinusoidal networks, and kernel methods.

\begin{thebibliography}{9}

\bibitem{sitzmann2020siren}
Sitzmann, V., Martel, J., Bergman, A., Lindell, D., \& Wetzstein, G. (2020).
\textit{Implicit neural representations with periodic activation functions}.
Advances in Neural Information Processing Systems, 33, 7462-7473.

\bibitem{rahimi2007random}
Rahimi, A., \& Recht, B. (2007).
\textit{Random features for large-scale kernel machines}.
Advances in Neural Information Processing Systems, 20.

\bibitem{mildenhall2020nerf}
Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., \& Ng, R. (2020).
\textit{NeRF: Representing scenes as neural radiance fields for view synthesis}.
European Conference on Computer Vision (ECCV).

\bibitem{zhang1992wavelet}
Zhang, Q., \& Benveniste, A. (1992).
\textit{Wavelet networks}.
IEEE Transactions on Neural Networks, 3(6), 889-898.

\bibitem{lee1996gabor}
Lee, T. S. (1996).
\textit{Image representation using 2D Gabor wavelets}.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(10), 959-971.

\end{thebibliography}

\appendix

\section{Implementation Details}

\subsection{PyTorch Implementation}

All architectures are implemented in PyTorch with custom \texttt{nn.Module} classes. Key implementation details:

\begin{itemize}
    \item \textbf{Envelope Width}: Parameterized as \texttt{log\_sigma} to ensure $\sigma > 0$
    \item \textbf{Optimization}: Adam with learning rate $10^{-3}$
    \item \textbf{Loss}: Mean Squared Error (MSE)
    \item \textbf{Initialization}: Xavier uniform for linear layers, Gaussian for wave parameters
\end{itemize}

\subsection{Computational Complexity}

For an ensemble of $n$ waves on $N$ data points in $d$ dimensions:

\textbf{Forward Pass}:
\begin{itemize}
    \item Phase computation: $O(nd)$
    \item Envelope computation: $O(nd)$
    \item Modulation: $O(nd)$
    \item Linear combination: $O(nN)$
    \item Total: $O(N(nd + n)) = O(Nnd)$
\end{itemize}

\textbf{Memory}: $O(Nn)$ for intermediate activations.

\subsection{Hyperparameter Selection}

\textbf{Ensemble Size $n$}:
\begin{itemize}
    \item 1D: 8-16 waves
    \item 4D: 8 waves
    \item 6D: 6 waves (reduced due to higher dimensional expressivity)
\end{itemize}

\textbf{Learning Rate}: $10^{-3}$ to $5 \times 10^{-3}$ depending on dimension

\textbf{Training Epochs}:
\begin{itemize}
    \item 1D: 1000-2000
    \item 4D: 300-1000
    \item 6D: 500
\end{itemize}

\section{Reproducibility}

All code, data, and experiments are available at:

\begin{center}
\url{https://github.com/QueenFi703/DREDGE}
\end{center}

Files included:
\begin{itemize}
    \item \texttt{quasimoto\_benchmark.py}: 1D benchmark
    \item \texttt{quasimoto\_extended\_benchmark.py}: 4D/6D + RFF + Interference
    \item \texttt{quasimoto\_6d\_benchmark.py}: Standalone 6D
    \item \texttt{quasimoto\_interference\_benchmark.py}: Complex-valued variant
    \item Documentation: Usage guides, experimentation ideas, resume content
\end{itemize}

\section{Additional Visualizations}

\subsection{Training Convergence}

Figure 1 shows convergence curves for Quasimoto, SIREN, and RFF on the 1D chirp signal. SIREN converges fastest but plateaus earliest. Quasimoto shows steady improvement throughout training.

\subsection{6D Projection}

Figure 2 visualizes a 2D slice of the learned 6D function at $x_3=x_4=x_5=0, t=0$, revealing localized Gaussian structures and multi-scale oscillatory patterns.

\end{document}
